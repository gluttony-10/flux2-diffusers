import io
import os
import gc
import json
from mmgp import offload, profile_type
import torch
import numpy as np
import gradio as gr
import socket
import psutil
import random
import argparse
import requests
import datetime
from diffusers import Flux2Pipeline, Flux2Transformer2DModel, QuantoConfig
from diffusers.utils import load_image


parser = argparse.ArgumentParser() 
parser.add_argument("--server_name", type=str, default="127.0.0.1", help="IPåœ°å€ï¼Œå±€åŸŸç½‘è®¿é—®æ”¹ä¸º0.0.0.0")
parser.add_argument("--server_port", type=int, default=7891, help="ä½¿ç”¨ç«¯å£")
parser.add_argument("--share", action="store_true", help="æ˜¯å¦å¯ç”¨gradioå…±äº«")
parser.add_argument("--mcp_server", action="store_true", help="æ˜¯å¦å¯ç”¨mcpæœåŠ¡")
parser.add_argument("--compile", action="store_true", help="æ˜¯å¦å¯ç”¨compileåŠ é€Ÿ")
parser.add_argument("--max_vram", type=float, default=0.8, help="æœ€å¤§æ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹")
args = parser.parse_args()

print(" å¯åŠ¨ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾… bilibili@åå­—é±¼ https://space.bilibili.com/893892")
print(f'\033[32mPytorchç‰ˆæœ¬ï¼š{torch.__version__}\033[0m')
if torch.cuda.is_available():
    device = "cuda" 
    print(f'\033[32mæ˜¾å¡å‹å·ï¼š{torch.cuda.get_device_name()}\033[0m')
    total_vram_in_gb = torch.cuda.get_device_properties(0).total_memory / 1073741824
    print(f'\033[32mæ˜¾å­˜å¤§å°ï¼š{total_vram_in_gb:.2f}GB\033[0m')
    mem = psutil.virtual_memory()
    print(f'\033[32må†…å­˜å¤§å°ï¼š{mem.total/1073741824:.2f}GB\033[0m')
    if torch.cuda.get_device_capability()[0] >= 8:
        print(f'\033[32mæ”¯æŒBF16\033[0m')
        dtype = torch.bfloat16
    else:
        print(f'\033[32mä¸æ”¯æŒBF16ï¼Œä»…æ”¯æŒFP16\033[0m')
        dtype = torch.float16
else:
    print(f'\033[32mCUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥\033[0m')
    device = "cpu"

os.makedirs("outputs", exist_ok=True)
repo_id = "./models"
budgets = int(torch.cuda.get_device_properties(0).total_memory/1048576 * args.max_vram)
stop_generation = False

"""quantization_config = QuantoConfig(weights_dtype="int8")
transformer = Flux2Transformer2DModel.from_pretrained(
    repo_id, 
    subfolder="transformer", 
    quantization_config=quantization_config,
    torch_dtype=dtype,
)
for name, param in transformer.named_parameters():
    if "input_scale" in name or "output_scale" in name:
        if param.shape == torch.Size([1]):
            param.data = param.data.squeeze()
transformer.save_pretrained("models/transformer-qint8")"""
transformer = Flux2Transformer2DModel.from_pretrained(
    repo_id, 
    subfolder="transformer-qint8", 
    torch_dtype=dtype,
    ignore_mismatched_sizes=True,
)
pipe = Flux2Pipeline.from_pretrained(
    repo_id, 
    text_encoder=None, 
    transformer=transformer,
    torch_dtype=dtype,
    low_cpu_mem_usage=False, 
)
mmgp = offload.profile(
    {"transformer": pipe.transformer, "vae": pipe.vae}, 
    profile_type.LowRAM_HighVRAM, 
    budgets={'*': budgets}, 
    compile=True if args.compile else False,
)

# è§£å†³å†²çªç«¯å£ï¼ˆæ„Ÿè°¢licyké…±æä¾›çš„ä»£ç ~ï¼‰
def find_port(port: int) -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        if s.connect_ex(("localhost", port)) == 0:
            print(f"ç«¯å£ {port} å·²è¢«å ç”¨ï¼Œæ­£åœ¨å¯»æ‰¾å¯ç”¨ç«¯å£...")
            return find_port(port=port + 1)
        else:
            return port


def save_token_to_file(token):
    """å°† token ä¿å­˜åˆ° token.json æ–‡ä»¶"""
    try:
        with open("token.json", "w") as f:
            json.dump({"hf_token": token}, f)
        return True
    except Exception as e:
        print(f"ä¿å­˜ token å¤±è´¥: {str(e)}")
        return False


def load_token_from_file():
    """ä» token.json æ–‡ä»¶åŠ è½½ tokenï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²"""
    try:
        if os.path.exists("token.json"):
            with open("token.json", "r") as f:
                data = json.load(f)
                return data.get("hf_token", "")
    except Exception as e:
        print(f"åŠ è½½ token å¤±è´¥: {str(e)}")
    return ""


def remote_text_encoder(prompts, hf_token):
    response = requests.post(
        "https://remote-text-encoder-flux-2.huggingface.co/predict",
        json={"prompt": prompts},
        headers={
            "Authorization": f"Bearer {hf_token}",
            "Content-Type": "application/json"
        }
    )
    prompt_embeds = torch.load(io.BytesIO(response.content))

    return prompt_embeds.to(device)
        

def exchange_width_height(width, height):
    return height, width, "âœ… å®½é«˜äº¤æ¢å®Œæ¯•"


def stop_generate():
    global stop_generation
    stop_generation = True
    return "ğŸ›‘ ç­‰å¾…ç”Ÿæˆä¸­æ­¢"


def scale_resolution_1_5(width, height):
    """
    å°†å®½åº¦å’Œé«˜åº¦éƒ½æ”¾å¤§1.5å€ï¼Œå¹¶æŒ‰ç…§16çš„å€æ•°å‘ä¸‹å–æ•´
    """
    new_width = int(width * 1.5) // 16 * 16
    new_height = int(height * 1.5) // 16 * 16
    return new_width, new_height, "âœ… åˆ†è¾¨ç‡å·²è°ƒæ•´ä¸º1.5å€"


def generate(
    prompt, 
    width, 
    height, 
    num_inference_steps, 
    batch_images, 
    seed_param, 
    hf_token,
    image_imput=None, 
):
    global stop_generation
    if hf_token.strip():
        save_token_to_file(hf_token)
    results = []
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    for i in range(batch_images):
        if stop_generation:
            stop_generation = False
            yield results, f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œæœ€åç§å­æ•°{seed+i-1}"
            break
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"outputs/{timestamp}.png"
        if image_imput:
            output = pipe(
                prompt_embeds=remote_text_encoder(prompt, hf_token),
                image=load_image(image_imput),
                generator=torch.Generator(device=device).manual_seed(seed),
                num_inference_steps=num_inference_steps,
                guidance_scale=4,
            )
        else:
            output = pipe(
                prompt_embeds=remote_text_encoder(prompt, hf_token),
                generator=torch.Generator(device=device).manual_seed(seed),
                num_inference_steps=num_inference_steps,
                guidance_scale=4,
            )
        image = output.images[0]
        image.save(filename)
        results.append(image)
        yield results, f"ç§å­æ•°{seed+i}ï¼Œä¿å­˜åœ°å€{filename}"
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
    

with gr.Blocks(title="flux2-diffusers") as demo:
    gr.Markdown("""
            <div>
                <h2 style="font-size: 30px;text-align: center;">flux2-diffusers</h2>
            </div>
            <div style="text-align: center;">
                åå­—é±¼
                <a href="https://space.bilibili.com/893892">ğŸŒbilibili</a> 
                |flux2-diffusers
                <a href="https://github.com/gluttony-10/flux2-diffusers">ğŸŒgithub</a> 
            </div>
            <div style="text-align: center; font-weight: bold; color: red;">
                âš ï¸ è¯¥æ¼”ç¤ºä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä½“éªŒä½¿ç”¨ã€‚
            </div>
            """)
    
    with gr.Tabs():
        with gr.TabItem("flux2-diffusers"):
            initial_token = load_token_from_file()
            hf_token = gr.Textbox(
                label="HuggingFace Token", 
                info="è¯·è¾“å…¥huggingface tokenï¼Œå¯[ç‚¹å‡»æ­¤å¤„](https://huggingface.co/settings/tokens)ç”³è¯·ï¼Œåˆ›å»ºæ—¶æƒé™é€‰â€œReadâ€å³å¯ã€‚",
                placeholder="è¯·è¾“å…¥HuggingFace Token...", 
                type="password",
                value=initial_token
            )
            with gr.Row():
                with gr.Column():
                    with gr.Accordion("å›¾åƒç¼–è¾‘ï¼ˆå¯é€‰ï¼‰", open=False):
                        image_input = gr.Image(type="pil", label="ä¸Šä¼ å›¾åƒ", height=300)
                    prompt = gr.Textbox(label="æç¤ºè¯", placeholder="è¯·è¾“å…¥æç¤ºè¯æŒ‡å¯¼è§†é¢‘ç”Ÿæˆ...")
                    generate_button = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                        with gr.Row():
                            width = gr.Slider(label="å®½åº¦", minimum=256, maximum=2656, step=16, value=1328)
                            height = gr.Slider(label="é«˜åº¦", minimum=256, maximum=2656, step=16, value=1328)
                        with gr.Row():
                            exchange_button = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜")
                            scale_1_5_button = gr.Button("1.5å€åˆ†è¾¨ç‡")
                        batch_images = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                        num_inference_steps = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è4æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=4)
                        seed_param = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=-1)
                with gr.Column():
                    info = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                    image_output = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                    stop_button = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
        
    gr.on(
        triggers=[generate_button.click, prompt.submit],
        fn = generate,
        inputs = [
            prompt,
            width,
            height,
            num_inference_steps,
            batch_images,
            seed_param,
            hf_token,
            image_input,
        ],
        outputs = [image_output, info]
    )
    exchange_button.click(
        fn=exchange_width_height, 
        inputs=[width, height], 
        outputs=[width, height, info]
    )
    scale_1_5_button.click(
        fn=scale_resolution_1_5,
        inputs=[width, height],
        outputs=[width, height, info]
    )


if __name__ == "__main__": 
    demo.launch(
        server_name=args.server_name, 
        server_port=find_port(args.server_port),
        share=args.share, 
        mcp_server=args.mcp_server,
        inbrowser=True,
        theme=gr.themes.Soft(font=[gr.themes.GoogleFont("IBM Plex Sans")]),
    )